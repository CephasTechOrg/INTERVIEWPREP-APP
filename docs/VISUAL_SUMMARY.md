# Interview Prep AI - Visual Summary & Cheat Sheet

## ğŸ¯ What This App Does (5-Second Version)

```
User logs in â†’ Creates interview session â†’
AI asks 5-7 questions (with follow-ups) â†’
User answers â†’ AI scores everything â†’
Shows results with feedback
```

---

## ğŸ—ï¸ System Components (Bird's Eye View)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER BROWSER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Next.js Frontend (React + TypeScript)                â”‚   â”‚
â”‚  â”‚ â€¢ Login/Signup pages                                 â”‚   â”‚
â”‚  â”‚ â€¢ Dashboard (create session)                         â”‚   â”‚
â”‚  â”‚ â€¢ Interview chat UI                                  â”‚   â”‚
â”‚  â”‚ â€¢ Results page with score breakdown                  â”‚   â”‚
â”‚  â”‚ State: Zustand (auth, session, UI)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/HTTPS + JWT Auth
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASTAPI BACKEND                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Core Services                                         â”‚   â”‚
â”‚  â”‚ â€¢ InterviewEngine: Pick questions, generate replies  â”‚   â”‚
â”‚  â”‚ â€¢ ScoringEngine: Evaluate and score interviews       â”‚   â”‚
â”‚  â”‚ â€¢ DeepSeekClient: Call LLM API with retries          â”‚   â”‚
â”‚  â”‚ â€¢ TTS: Generate audio for playback                   â”‚   â”‚
â”‚  â”‚ â€¢ RAG: Find similar sessions for context             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ SQL
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              POSTGRESQL DATABASE                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Tables:                                               â”‚   â”‚
â”‚  â”‚ â€¢ users, sessions, questions, messages               â”‚   â”‚
â”‚  â”‚ â€¢ evaluations, embeddings, feedback, audit logs      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†‘
        â”‚ External APIs
        â”œâ”€ DeepSeek LLM
        â”œâ”€ ElevenLabs TTS (optional)
        â””â”€ SMTP Email (optional)
```

---

## ğŸ“– User Journey (Step by Step)

```
1. SIGNUP
   User enters email + password + name
        â†“ Sends verification code
   User enters 6-digit code
        â†“ Creates account
   Redirects to login

2. LOGIN
   User enters email + password
        â†“ Returns JWT token
   Token stored in browser localStorage
   Redirects to dashboard

3. CREATE SESSION
   User selects:
      â€¢ Role: "SWE Intern" / "SWE Engineer" / etc
      â€¢ Track: "swe_intern" / "data_science" / etc
      â€¢ Company: "Google" / "Apple" / "General"
      â€¢ Difficulty: "Easy" / "Medium" / "Hard"
        â†“ API creates InterviewSession row
   Returns session ID + start button

4. WARMUP (First message)
   User clicks "Start Interview"
        â†“ Backend generates greeting
   AI: "Hi, I'm Sarah from Google. How are you today?"
   User: "I'm doing well!"

5. MAIN INTERVIEW LOOP (5-7 questions)
   Backend picks next question (matching params + diversity)
   AI: "Question: Given an array of integers, find the two numbers..."
   User types/speaks answer (with code block if needed)
        â†“ LLM scores response on 5 dimensions
        â†“ Stores running average of scores

   Option A: AI asks follow-up
      AI: "Good! Can you explain the time complexity?"
      User: "It's O(n) because..."

   Option B: Move to next question
      AI: "Great. Next question..."

   [Repeat for 5-7 questions]

6. FINALIZATION
   User clicks "End Interview"
        â†“ Backend collects all messages
        â†“ LLM scores entire interview
   Returns: overall_score (0-100) + rubric breakdown

7. RESULTS
   Frontend displays:
      â€¢ Overall Score: 72/100
      â€¢ Rubric: {communication: 7, problem_solving: 7, ...}
      â€¢ Strengths: ["Clear explanations", ...]
      â€¢ Weaknesses: ["Missed edge case", ...]
      â€¢ Next Steps: ["Practice system design", ...]

8. HISTORY
   User can see all past sessions
   Compare scores over time
```

---

## ğŸ” Data Flow: Authentication

```
BROWSER                          API SERVER
  â”‚                               â”‚
  â”œâ”€ POST /auth/signup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
  â”‚  {email, password, name}      â”‚ Hash password (Argon2)
  â”‚                               â”‚ Create pending_signup row
  â”‚                               â”‚ Send verification code email
  â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ {ok: true} â”€â”€â”€â”€â”€â”¤
  â”‚                               â”‚
  â”œâ”€ POST /auth/verify â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
  â”‚  {email, code: "123456"}      â”‚ Validate code
  â”‚                               â”‚ Create User row
  â”‚  â†â”€â”€â”€â”€â”€â”€â”€ {access_token} â”€â”€â”€â”€â”€â”¤ Generate JWT
  â”‚                               â”‚
  â”‚ Store token in localStorage   â”‚
  â”‚ (key: "access_token")         â”‚
  â”‚                               â”‚
  â”œâ”€ Subsequent API calls:        â”‚
  â”œâ”€ GET /api/v1/sessions â”€â”€â”€â”€â”€â”€â”€â†’ Authorization: Bearer <TOKEN>
  â”‚                               â”‚ Decode JWT â†’ get email â†’ fetch user
  â”‚                               â”‚ Attach user to request context
  â”‚  â†â”€â”€â”€â”€â”€â”€â”€ {sessions: [...]} â”€â”€â”¤
```

---

## ğŸ® Interview Engine: Question Selection Algorithm

```
When Backend Needs Next Question:
  â”‚
  â”œâ”€ 1. Build pool of candidates:
  â”‚      Questions where:
  â”‚      â€¢ track = session.track (e.g., "swe_intern")
  â”‚      â€¢ company_style = session.company_style
  â”‚      â€¢ difficulty = session.difficulty_current
  â”‚      â€¢ NOT already_seen by this user
  â”‚
  â”œâ”€ 2. Apply tag diversity:
  â”‚      â€¢ Track tags_seen in running scores
  â”‚      â€¢ Prefer questions with NEW tags
  â”‚      â€¢ Avoid duplicate tag combinations
  â”‚
  â”œâ”€ 3. Check behavioral quota:
  â”‚      if behavioral_questions_asked < behavioral_target:
  â”‚         Pick behavioral question
  â”‚      else:
  â”‚         Pick technical question
  â”‚
  â”œâ”€ 4. Adjust difficulty:
  â”‚      Compute running skill average
  â”‚      if avg_score > 7.5: difficulty_current = "hard"
  â”‚      elif avg_score > 5: difficulty_current = "medium"
  â”‚      else: difficulty_current = "easy"
  â”‚
  â””â”€ 5. Return selected question
```

---

## ğŸ“Š Message Flow: User Answers Question

```
User types answer in chat input
         â†“
User clicks "Send" button
         â†“
Frontend validates input (not empty, < 5000 chars)
         â†“
POST /sessions/{session_id}/message
  {
    "input_mode": "text",
    "content": "I would use a HashMap to store indices..."
  }
         â†“
BACKEND processes:
  1. Store message in DB (role="student")

  2. Compute quick rubric score:
     LLM or rule-based scoring:
     {
       "communication": 7,
       "problem_solving": 6,
       "correctness_reasoning": 7,
       "complexity": 8,
       "edge_cases": 5
     }

  3. Update skill_state running average:
     skill_state = {
       "n": 3,  // number of responses
       "sum": {...scores summed...},
       "last": {...latest scores...}
     }

  4. Check for special cases:
     â€¢ Did user say "I don't know"? â†’ Simple follow-up
     â€¢ Was answer too vague? â†’ Prompt for detail
     â€¢ Is user off-topic? â†’ Redirect with reanchoring

  5. Generate AI follow-up or next question:
     Use LLM with prompts or fallback dataset-driven followups

  6. Store interviewer response in DB

  7. Check if max followups reached
     if followups_used >= max:
       Mark ready for next question

  â†“
Response: {
  "role": "interviewer",
  "content": "Great! Can you walk me through the time complexity...",
  "ready_for_next": false
}
         â†“
Frontend receives and displays in chat
```

---

## ğŸ† Interview Finalization: Scoring Process

```
When interview ends (max_questions reached):
  â”‚
  â”œâ”€ 1. Collect transcript:
  â”‚      All messages from session.messages
  â”‚      Format as alternating INTERVIEWER/CANDIDATE lines
  â”‚
  â”œâ”€ 2. Build scoring context:
  â”‚      â€¢ Rubric for evaluated dimensions
  â”‚      â€¢ Whether behavioral questions included
  â”‚      â€¢ Similar session context (RAG)
  â”‚
  â”œâ”€ 3. Call LLM to score:
  â”‚      System Prompt: "You are an expert tech interviewer..."
  â”‚      User Prompt: "Score this interview transcript..."
  â”‚
  â”œâ”€ 4. Parse LLM response:
  â”‚      {
  â”‚        "overall_score": 72,
  â”‚        "rubric": {
  â”‚          "communication": 7,
  â”‚          "problem_solving": 7,
  â”‚          "correctness_reasoning": 6,
  â”‚          "complexity": 8,
  â”‚          "edge_cases": 5
  â”‚        },
  â”‚        "strengths": ["Clear explanation", ...],
  â”‚        "weaknesses": ["Missed edge case", ...],
  â”‚        "next_steps": ["Practice system design", ...]
  â”‚      }
  â”‚
  â”œâ”€ 5. Calibrate score:
  â”‚      Adjust overall_score based on rubric signals
  â”‚      Ensure consistency (don't award 100 if rubric avg is 5)
  â”‚
  â”œâ”€ 6. Store evaluation:
  â”‚      CREATE evaluation row with scores
  â”‚
  â”œâ”€ 7. Trigger embedding generation:
  â”‚      Convert session to embedding for RAG
  â”‚      Store in session_embedding table
  â”‚
  â””â”€ 8. Return evaluation to frontend
         Frontend displays score + breakdown
```

---

## ğŸ—„ï¸ Data Model Relationships

```
USER
  â”œâ”€ 1:Many â”€â”€â†’ INTERVIEW_SESSION
  â”‚              â”œâ”€ 1:Many â”€â”€â†’ MESSAGE
  â”‚              â”‚              â””â”€ role: interviewer|student
  â”‚              â”œâ”€ 1:Many â”€â”€â†’ SESSION_QUESTION
  â”‚              â”‚              â””â”€ FK â†’ QUESTION
  â”‚              â”œâ”€ 1:One â”€â”€â†’ EVALUATION
  â”‚              â”‚             â”œâ”€ overall_score: 0-100
  â”‚              â”‚             â”œâ”€ rubric: JSON
  â”‚              â”‚             â””â”€ summary: JSON
  â”‚              â””â”€ 1:Many â”€â”€â†’ SESSION_EMBEDDING
  â”‚                             â””â”€ vector embedding
  â”‚
  â””â”€ 1:Many â”€â”€â†’ USER_QUESTION_SEEN
                 â””â”€ Tracks which questions user already saw
                    (prevents repeats)

QUESTION
  â”œâ”€ 1:Many â”€â”€â†’ SESSION_QUESTION
  â””â”€ Properties:
     â”œâ”€ track (swe_intern, data_science, etc)
     â”œâ”€ company_style (google, apple, general)
     â”œâ”€ difficulty (easy, medium, hard)
     â”œâ”€ tags_csv (arrays, sorting, medium, etc)
     â”œâ”€ question_type (coding, system_design, behavioral)
     â””â”€ followups (JSON - optional)
```

---

## ğŸ”§ Tech Stack Summary

| Layer      | Technology            | Purpose                   |
| ---------- | --------------------- | ------------------------- |
| Frontend   | Next.js 16 + React 19 | Web UI, routing, SSR      |
| State      | Zustand               | Client-side state         |
| API Client | Axios                 | HTTP requests with auth   |
| Backend    | FastAPI               | REST API framework        |
| Database   | PostgreSQL            | Persistent data storage   |
| ORM        | SQLAlchemy            | Pythonâ†”SQL mapping        |
| Migrations | Alembic               | Schema versioning         |
| Auth       | PyJWT + Argon2        | Token + password security |
| LLM        | DeepSeek API          | Interview generation      |
| TTS        | ElevenLabs + fallback | Audio generation          |
| Testing    | Pytest + Vitest       | Unit tests                |

---

## ğŸš€ Running the Full System

```bash
# Terminal 1: Database
docker-compose up -d          # Starts PostgreSQL

# Terminal 2: Backend
cd backend
python -m venv .venv
. .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt
alembic upgrade head          # Apply migrations
python seed.py --questions    # Load questions
uvicorn app.main:app --reload # Start server @ http://127.0.0.1:8000

# Terminal 3: Frontend
cd frontend-next
npm install
npm run dev                    # Start @ http://localhost:3000

# Open browser â†’ http://localhost:3000
# Sign up â†’ Create session â†’ Start interview!
```

---

## ğŸ§  Key Concepts

### 1. Adaptive Difficulty

- Start: easy
- Track running score average
- Adjust question pool difficulty based on performance

### 2. Tag Diversity

- Ensure questions cover different topics
- Avoid asking multiple "array" questions
- Maintain breadth across domains

### 3. Behavioral Mix

- Target 2-3 behavioral questions per session
- Mix with technical questions
- Behavioral score influences overall score

### 4. LLM Fallback

- If DeepSeek unavailable: use dataset-driven fallbacks
- If no response: return conservative generic follow-ups
- Score fallback: simple rules instead of LLM

### 5. RAG (Retrieval-Augmented Generation)

- After interview finalized: embed session as vector
- Retrieve similar past sessions
- Use context to improve evaluation consistency

### 6. Skill State Tracking

```python
skill_state = {
  "n": 3,  # responses scored so far
  "sum": {  # cumulative scores
    "communication": 20,
    "problem_solving": 18,
    ...
  },
  "last": {  # scores from latest response
    "communication": 7,
    "problem_solving": 6,
    ...
  },
  "pool": {...question pool metadata...},
  "interviewer": {...interviewer profile...},
  "intro_used": true  # track greeting sent
}
```

---

## ğŸ“ˆ Expected Interview Timeline

| Phase          | Duration      | Events                    |
| -------------- | ------------- | ------------------------- |
| Warmup         | 1-2 min       | Greeting + small talk     |
| Question 1     | 3-5 min       | Question + 1-2 follow-ups |
| Question 2     | 3-5 min       | Question + 1-2 follow-ups |
| Question 3     | 3-5 min       | Question + 1-2 follow-ups |
| Question 4     | 3-5 min       | Question + 1-2 follow-ups |
| Question 5     | 3-5 min       | Question + 1-2 follow-ups |
| (Optional 6-7) | 3-5 min       | Additional questions      |
| **Total**      | **25-40 min** | Complete interview        |
| Finalization   | 5-30 sec      | LLM scoring               |

---

## âš¡ Performance Characteristics

| Operation          | Time  | Notes                    |
| ------------------ | ----- | ------------------------ |
| Signup             | 500ms | Email sending async      |
| Login              | 200ms | JWT verification         |
| Create session     | 500ms | DB write                 |
| Load messages      | 200ms | < 100 messages           |
| Send message       | 3-15s | Includes LLM call        |
| Finalize           | 5-30s | Full transcript LLM eval |
| Generate embedding | 2-5s  | After finalization       |

---

## ğŸ“ Code Paths for Common Tasks

### "I want to add a new question type"

1. Update `Question.question_type` enum in model
2. Add rubric section in `rubric_loader.py`
3. Update question selection logic in `interview_engine.py`
4. Create migration: `alembic revision --autogenerate -m "add question type"`

### "I want to customize scoring"

1. Edit `scoring_engine.py` â†’ `_fallback_evaluation_data()`
2. Modify prompt in `prompt_templates.py` â†’ `evaluator_system_prompt()`
3. Adjust calibration in `_calibrate_overall_score()`

### "I want to add a new difficulty level"

1. Update `ALLOWED_DIFFICULTIES` in `backend/app/core/constants.py`
2. Add difficulty tier to question datasets in `data/questions/`
3. Update frontend form validation

### "I want to integrate a different LLM"

1. Create new client in `backend/app/services/llm_client.py`
2. Implement `chat()` and `chat_json()` methods
3. Update imports in `interview_engine.py` and `scoring_engine.py`

---

## ğŸ“ Debugging Checklist

```
[ ] Is backend running? (check http://127.0.0.1:8000/health)
[ ] Is frontend running? (check http://localhost:3000)
[ ] Is PostgreSQL running? (check docker ps)
[ ] Are migrations applied? (alembic current)
[ ] Is DATABASE_URL correct? (check .env)
[ ] Is token in localStorage? (DevTools â†’ Application)
[ ] Does token have correct user? (JWT.io â†’ decode)
[ ] Is LLM API key set? (check /api/v1/ai/status)
[ ] Are questions seeded? (check database)
[ ] Are CORS origins correct? (check /api/v1/sessions â†’ preflight)
```

---

## ğŸ¯ Project Goals Achieved

âœ… Realistic interview simulation  
âœ… Dynamic question selection  
âœ… AI-powered follow-ups  
âœ… Comprehensive scoring  
âœ… Local deployment  
âœ… JWT authentication  
âœ… Session history & analytics  
âœ… Adaptive difficulty  
âœ… Fallback mechanisms  
âœ… Production-ready architecture

---

**Print this page as reference while working on the project!**
