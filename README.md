# Interview Prep AI

## Overview

Interview Prep AI is a local, full-stack mock interview app. It pairs a FastAPI backend (question selection, LLM-driven interviewer, scoring) with a static HTML/CSS/JS frontend. The system is built to run locally with Postgres, seed questions automatically from JSON files, and let candidates practice technical + behavioral interviews with dynamic follow-ups.

Key goals:

- Realistic interview flow (warmup, behavioral, technical, follow-ups, scoring).
- Dynamic question selection (difficulty, tags, company style).
- Simple local setup (no frontend build step).

## Architecture

High-level flow:

Frontend (static HTML/JS)
-> fetch JSON to API_BASE with Authorization: Bearer <token>
-> renders sessions, questions, chat, results

Backend (FastAPI)
-> /api/v1/auth for signup/login/verification
-> /api/v1/sessions for interview flow
-> /api/v1/questions for datasets
-> /api/v1/analytics for results
-> /api/v1/ai for LLM status
-> /api/v1/tts for audio
-> SQLAlchemy models + Postgres
-> LLM (DeepSeek) with fallback

Database (Postgres)
-> users, sessions, messages, questions, evaluations, session_questions, user_questions_seen

## Directory Structure

- backend/ FastAPI backend + SQLAlchemy
- frontend/ Static HTML/CSS/JS
- data/questions/ Question datasets (JSON) loaded into Postgres
- scripts/ Validation and regression scripts
- docker-compose.yml Local Postgres container

## Backend Details

Entry point: `backend/app/main.py`

- Uses Alembic for database migrations (see `backend/MIGRATIONS.md`).
- Seeds questions from `data/questions/` if the DB is empty.
- Enables permissive CORS for local development.

Database Migrations:

- Alembic is configured for versioned schema management.
- See `backend/MIGRATIONS.md` for detailed migration guide.
- Initialize migrations: `cd backend && python scripts/init_migrations.py`
- Apply migrations: `cd backend && alembic upgrade head`

Important services:

- `InterviewEngine` (`backend/app/services/interview_engine.py`)
  - Warmup behavior and greeting.
  - Picks behavioral + technical questions.
  - Uses adaptive difficulty and tag diversity.
  - Generates follow-ups with the LLM or offline fallback.
- `ScoringEngine` (`backend/app/services/scoring_engine.py`)
  - Summarizes the transcript into rubric + strengths/weaknesses.
  - Uses LLM with a conservative fallback if AI is unavailable.
- `LLM client` (`backend/app/services/llm_client.py`)
  - DeepSeek chat API with retry/backoff and health tracking.
  - Exposes `/api/v1/ai/status` for the frontend badge.

Authentication:

- Signup -> verification token (email or console print).
- Login returns a JWT; frontend stores it in localStorage.
- All API routes require `Authorization: Bearer <token>`.

Email:

- If SMTP is not configured, verification tokens are printed to the backend console.

## Frontend Details

Entry pages:

- `frontend/login.html` for authentication and account management.
- `frontend/interview.html` for the main app (dashboard, history, interview, results).

API client:

- `frontend/assets/js/api.js` defines `API_BASE` (default `http://127.0.0.1:8000/api/v1`).
- Updates to the backend URL should happen here.

State:

- JWT is stored in localStorage (`token`).
- Session ID is stored in localStorage (`current_session_id`).
- Profile preferences are stored locally.

## Interview Flow (End-to-End)

1. User logs in (JWT stored in localStorage).
2. User starts a session from the dashboard.
3. Backend creates a session row and starts the warmup.
4. Warmup:
   - Greeting + "I am doing well."
   - 1 behavioral prompt based on company style.
5. Main interview:
   - The engine selects a question based on track, company, difficulty, tag diversity.
   - Follow-ups are generated by the LLM or a deterministic fallback.
6. Finalize:
   - Transcript is scored.
   - Results returned via `/api/v1/analytics/sessions/{id}/results`.

## How the Backend and Frontend Communicate

- Frontend makes JSON requests to `API_BASE`.
- Auth header: `Authorization: Bearer <token>`.
- Backend responds with JSON objects for sessions, messages, questions, and results.

Core endpoints:

- POST `/api/v1/auth/signup`
- POST `/api/v1/auth/verify`
- POST `/api/v1/auth/login`
- POST `/api/v1/sessions`
- POST `/api/v1/sessions/{id}/start`
- POST `/api/v1/sessions/{id}/message`
- POST `/api/v1/sessions/{id}/finalize`
- GET `/api/v1/questions/coverage`
- GET `/api/v1/analytics/sessions/{id}/results`
- GET `/api/v1/ai/status`
- POST `/api/v1/tts`

## Configuration

Create `backend/.env` (use `backend/.env.example` as a template).

Required:

- `SECRET_KEY`
- `DATABASE_URL`

Optional (AI + TTS):

- `DEEPSEEK_API_KEY`, `DEEPSEEK_BASE_URL`, `DEEPSEEK_MODEL`
- `TTS_PRIMARY`, `TTS_FALLBACK`, `ELEVENLABS_API_KEY`, `ELEVENLABS_VOICE_ID`

Optional (email):

- `SMTP_HOST`, `SMTP_PORT`, `SMTP_USERNAME`, `SMTP_PASSWORD`, `SMTP_FROM`, `SMTP_TLS`

Note: The backend always loads `backend/.env` directly (even if you run from the repo root).

## Development Setup

This project uses professional development tools for code quality, testing, and database management.

### Database Migrations (Alembic)

The project uses Alembic for versioned database schema management.

**Initialize database:**

```bash
cd backend
python scripts/init_migrations.py  # Creates initial migration
alembic upgrade head              # Applies all migrations
```

**Migration commands:**

```bash
cd backend

# Create new migration after schema changes
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Check current migration status
alembic current

# View migration history
alembic history
```

**Important:** Always run migrations before starting the backend.

### Testing (Pytest)

Comprehensive test suite covering backend functionality.

**Run tests:**

```bash
cd backend

# Run all tests
pytest

# Run specific test categories
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests
pytest -m "not slow"    # Skip slow tests

# Run with coverage
pytest --cov=app --cov-report=html

# Run specific test file
pytest tests/test_api_endpoints.py
```

**Test structure:**

- `tests/test_api_endpoints.py` - API endpoint tests
- `tests/test_interview_engine.py` - Interview logic tests
- `tests/test_llm_client.py` - LLM integration tests
- `tests/test_auth.py` - Authentication tests
- `tests/test_crud.py` - Database CRUD tests

### CI/CD Pipeline

GitHub Actions automatically runs quality checks on every push and pull request.

**What it checks:**

- **Code formatting** with Black
- **Linting** with Ruff
- **Type checking** with MyPy
- **Database migrations** status
- **Question dataset validation**
- **Test execution** with coverage reporting
- **Security scanning** with bandit
- **Frontend validation**

**View CI results:**

- Go to the "Actions" tab in GitHub
- Click on the latest workflow run
- Check test results and coverage reports

## Run Locally

1. **Development Setup (One-time):**

   - Install development dependencies: `pip install pre-commit`
   - Set up pre-commit hooks: `pre-commit install`

2. **Start Database:**

   - `docker-compose up -d`

3. **Initialize Database (First time only):**

   - `cd backend`
   - `python scripts/init_migrations.py` # Creates initial migration
   - `alembic upgrade head` # Applies migrations

4. **Backend:**

   - `cd backend`
   - `python -m venv .venv`
   - Windows: `.\.venv\Scripts\activate`
   - macOS/Linux: `source .venv/bin/activate`
   - `pip install -r requirements.txt`
   - `pip install -r requirements-dev.txt` # For development tools
   - `uvicorn app.main:app --reload`

5. **Frontend:**

   - `cd frontend`
   - `python -m http.server 5173`
   - Open `http://127.0.0.1:5173/login.html`

6. **Run Tests (Optional):**
   - `cd backend && pytest` # Run test suite

## Data and Questions

Questions are stored in JSON and loaded into Postgres on backend startup:

- `data/questions/<track>/<company>/<difficulty>.json`
- `track` is usually `swe_intern` or `swe_engineer`.
- `company` can be `general`, `amazon`, `google`, `apple`, `microsoft`, `meta`.
- `difficulty` is `easy`, `medium`, `hard`.
- Behavioral questions are indicated by a `behavioral` tag.

Validation:

- Run `python scripts/validate_questions.py` to check tags and structure.

## Updating the System

Add or update questions:

1. Edit JSON in `data/questions/...`.
2. Ensure `company_style`, `track`, and `difficulty` match the allowed values in:
   - `backend/app/core/constants.py`
3. Run `python scripts/validate_questions.py`.
4. Restart the backend (it will load new questions if DB is empty; otherwise insert manually).

Adjust interview behavior:

- Prompts: `backend/app/services/prompt_templates.py`
- Question selection logic: `backend/app/services/interview_engine.py`
- Scoring rubric: `backend/app/services/rubric_loader.py`

Adjust frontend:

- Layout + UI: `frontend/assets/css/pro.css`
- Responsive rules: `frontend/assets/css/responsive.css`
- App logic: `frontend/assets/js/interview.js`
- API base URL: `frontend/assets/js/api.js`

## Troubleshooting

- "AI: Offline" badge:
  - Set `DEEPSEEK_API_KEY` in `backend/.env`, restart backend.
- Signup verification not received:
  - If SMTP is not configured, the token is printed in the backend console.
- 401 errors in the frontend:
  - Log in again to refresh the JWT in localStorage.
- Questions not showing:
  - Ensure Postgres is running and `data/questions` is valid.
  - Check backend logs for seeding messages.
